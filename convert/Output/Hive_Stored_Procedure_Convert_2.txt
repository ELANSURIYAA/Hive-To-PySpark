=============================================
Author:        Ascendion AVA+
Created on:   
Description:   Conversion of Hive stored procedure for processing sales data with aggregations and summary operations to PySpark DataFrame API
=============================================

# PySpark Conversion of Hive Stored Procedure: process_sales_data

## Original Hive Stored Procedure Analysis:
The original Hive stored procedure performs the following operations:
1. Takes start_date and end_date as input parameters
2. Creates dynamic SQL to insert aggregated sales data into summary_table
3. Creates a temporary table for sales summary
4. Uses cursor operations to iterate through results
5. Inserts data into detailed_sales_summary table
6. Cleans up temporary table

## PySpark DataFrame Conversion:

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import sum as spark_sum, col
from pyspark.sql.types import *
from datetime import datetime

# Initialize Spark Session
spark = SparkSession.builder \
    .appName("SalesDataProcessor") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .getOrCreate()

def process_sales_data(start_date: str, end_date: str):
    """
    PySpark equivalent of the Hive stored procedure process_sales_data
    
    Args:
        start_date (str): Start date in string format (YYYY-MM-DD)
        end_date (str): End date in string format (YYYY-MM-DD)
    
    Returns:
        None: Processes data and writes to target tables
    """
    
    try:
        # Read the sales_table (assuming it's already available as a DataFrame or table)
        # If reading from a file system, adjust the path accordingly
        sales_df = spark.table("sales_table")
        
        # Filter data based on date range (equivalent to WHERE clause in Hive)
        filtered_sales = sales_df.filter(
            (col("sale_date") >= start_date) & 
            (col("sale_date") <= end_date)
        )
        
        # Create aggregated summary (equivalent to the dynamic query and temp table)
        sales_summary = filtered_sales.groupBy("product_id") \
            .agg(spark_sum("sales").alias("total_sales")) \
            .select("product_id", "total_sales")
        
        # Cache the summary for multiple operations (performance optimization)
        sales_summary.cache()
        
        # Insert into summary_table (equivalent to the dynamic INSERT statement)
        # Mode 'append' adds new records, use 'overwrite' if you want to replace
        sales_summary.write \
            .mode("append") \
            .insertInto("summary_table")
        
        # Alternative approach using SQL if summary_table exists:
        # sales_summary.createOrReplaceTempView("temp_summary_view")
        # spark.sql("""
        #     INSERT INTO summary_table 
        #     SELECT product_id, total_sales 
        #     FROM temp_summary_view
        # """)
        
        # Process each record (equivalent to cursor operations)
        # In PySpark, we can process all records at once instead of row-by-row
        
        # Collect results for row-by-row processing if absolutely necessary
        # Note: collect() brings all data to driver - use only for small datasets
        summary_records = sales_summary.collect()
        
        # Create a list to store records for batch insert
        detailed_records = []
        
        for row in summary_records:
            product_id = row['product_id']
            total_sales = row['total_sales']
            
            # Add to detailed records list
            detailed_records.append((product_id, total_sales))
        
        # Convert list to DataFrame for batch insert (more efficient than row-by-row)
        if detailed_records:
            detailed_schema = StructType([
                StructField("product_id", StringType(), True),
                StructField("total_sales", FloatType(), True)
            ])
            
            detailed_df = spark.createDataFrame(detailed_records, detailed_schema)
            
            # Insert into detailed_sales_summary table
            detailed_df.write \
                .mode("append") \
                .insertInto("detailed_sales_summary")
        
        # Alternative: Direct DataFrame operation without collect (recommended approach)
        # This is more efficient and leverages Spark's distributed processing
        sales_summary.select("product_id", "total_sales") \
            .write \
            .mode("append") \
            .insertInto("detailed_sales_summary")
        
        # Unpersist cached DataFrame to free memory
        sales_summary.unpersist()
        
        print(f"Successfully processed sales data from {start_date} to {end_date}")
        
    except Exception as e:
        print(f"Error processing sales data: {str(e)}")
        raise e

# Alternative implementation using pure DataFrame operations (Recommended)
def process_sales_data_optimized(start_date: str, end_date: str):
    """
    Optimized PySpark implementation leveraging distributed processing
    
    Args:
        start_date (str): Start date in string format
        end_date (str): End date in string format
    """
    
    # Read sales data
    sales_df = spark.table("sales_table")
    
    # Create the aggregated summary with date filtering
    sales_summary = sales_df \
        .filter((col("sale_date") >= start_date) & (col("sale_date") <= end_date)) \
        .groupBy("product_id") \
        .agg(spark_sum("sales").alias("total_sales")) \
        .select("product_id", "total_sales")
    
    # Write to both target tables in a single operation
    # Insert into summary_table
    sales_summary.write.mode("append").insertInto("summary_table")
    
    # Insert into detailed_sales_summary
    sales_summary.write.mode("append").insertInto("detailed_sales_summary")
    
    print(f"Optimized processing completed for date range: {start_date} to {end_date}")

# Usage example:
if __name__ == "__main__":
    # Example usage
    start_date = "2023-01-01"
    end_date = "2023-12-31"
    
    # Call the function
    process_sales_data_optimized(start_date, end_date)
    
    # Stop Spark session
    spark.stop()
```

## Key Conversion Notes:

### 1. **Parameter Handling**:
- Hive stored procedure parameters are converted to Python function parameters
- Input validation can be added as needed

### 2. **Dynamic SQL Conversion**:
- Hive's dynamic SQL with CONCAT and EXECUTE IMMEDIATE is replaced with PySpark DataFrame operations
- More type-safe and easier to debug

### 3. **Temporary Table Handling**:
- Hive's CREATE TEMPORARY TABLE is replaced with DataFrame caching
- Temporary views can be used if SQL-like operations are preferred

### 4. **Cursor Operations**:
- Hive cursors are replaced with DataFrame operations
- Two approaches provided: collect() for row-by-row processing and direct DataFrame operations (recommended)

### 5. **Performance Optimizations**:
- DataFrame caching for reused data
- Batch operations instead of row-by-row processing
- Adaptive query execution enabled
- Proper resource cleanup

### 6. **Error Handling**:
- Try-catch blocks for robust error handling
- Proper resource cleanup in case of failures

### 7. **Memory Management**:
- Explicit unpersist() calls to free cached data
- Avoiding unnecessary collect() operations on large datasets

## Alternative SQL-based Approach:

If you prefer to keep some SQL syntax, you can use Spark SQL:

```python
def process_sales_data_sql(start_date: str, end_date: str):
    """
    SQL-based approach using Spark SQL
    """
    
    # Create temporary view for easier SQL operations
    spark.table("sales_table").createOrReplaceTempView("sales_view")
    
    # Execute SQL query
    summary_sql = f"""
        SELECT product_id, SUM(sales) as total_sales
        FROM sales_view
        WHERE sale_date BETWEEN '{start_date}' AND '{end_date}'
        GROUP BY product_id
    """
    
    sales_summary = spark.sql(summary_sql)
    
    # Insert into target tables
    sales_summary.write.mode("append").insertInto("summary_table")
    sales_summary.write.mode("append").insertInto("detailed_sales_summary")
```

## Testing and Validation:

```python
# Unit test example
def test_process_sales_data():
    # Create test data
    test_data = [
        ("P001", "2023-06-15", 100.0),
        ("P001", "2023-06-16", 150.0),
        ("P002", "2023-06-15", 200.0)
    ]
    
    test_schema = StructType([
        StructField("product_id", StringType(), True),
        StructField("sale_date", StringType(), True),
        StructField("sales", FloatType(), True)
    ])
    
    test_df = spark.createDataFrame(test_data, test_schema)
    test_df.createOrReplaceTempView("sales_table")
    
    # Run the function
    process_sales_data_optimized("2023-06-15", "2023-06-16")
    
    # Validate results
    result = spark.table("summary_table")
    assert result.count() > 0, "Summary table should contain data"
```

This conversion maintains the original logic while leveraging PySpark's distributed computing capabilities for better performance and scalability.